<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Accessible musical instruments and interfaces</title>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 0 auto;
            max-width: 900px;
            padding: 2em;
        }

        h1,
        h2,
        h3,
        h4 {
            color: #333;
            line-height: 1.2;
        }

        h1 {
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }

        h2 {
            border-bottom: 1px solid #eee;
            padding-bottom: 8px;
            margin-top: 2.5em;
        }

        h3 {
            margin-top: 2em;
            border-bottom: none;
        }

        h4 {
            margin-top: 1.5em;
        }

        a {
            color: #005a9c;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        img,
        iframe {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
            border: 1px solid #ddd;
            padding: 4px;
            border-radius: 4px;
        }

        /* Override iframe height for YouTube videos */
        iframe[src*="youtube.com"] {
            height: 315px !important;
            min-height: 315px;
        }

        .container {
            padding: 1em;
        }

        .menu ul {
            list-style-type: none;
            padding: 0;
        }

        .menu>ul>li {
            display: inline-block;
            margin-right: 15px;
            vertical-align: top;
        }

        .menu ul ul li {
            display: block;
            margin: 5px 0;
            font-size: 0.9em;
        }

        hr {
            border: none;
            border-top: 2px solid #eee;
            margin: 2em 0;
        }

        p,
        ul,
        li {
            color: #444;
        }

        #publications li {
            margin-bottom: 1em;
        }

        cite {
            display: block;
            text-align: center;
            font-style: italic;
            color: #777;
            margin-top: -1em;
            margin-bottom: 1.5em;
        }

        .indented-section {
            padding-left: 20px;
            margin-top: 30px;
            padding-bottom: 1px;
            margin-bottom: 20px;
        }
    </style>
</head>

<body>

    <div class="container">
        <h1>Accessible musical instruments and interfaces</h1>
        <nav class="menu">
            <strong>Go to:</strong>
            <ul>
                <li><a href="#project-description">Project Description</a></li>
                <li><a href="#instruments">Musical Instruments</a>
                    <ul style="padding-left: 20px; margin-top: 5px;">
                        <li><a href="#netytar">Netytar</a></li>
                        <li><a href="#netychords">Netychords</a></li>
                        <li><a href="#resin">Resin</a></li>
                        <li><a href="#kiroll">Kiroll</a></li>
                        <li><a href="#djeye">DJeye</a></li>
                    </ul>
                </li>
                <li><a href="#nith-framework">The NITH Framework</a>
                    <ul style="padding-left: 20px; margin-top: 5px;">
                        <li><a href="#nithsensors">NITHsensors (Hardware)</a></li>
                        <li><a href="#nithsoftware">NITHwrappers & Libraries</a></li>
                    </ul>
                </li>
                <li><a href="#publications">Publications</a></li>
            </ul>
        </nav>

        <hr>

        <section id="project-description">
            <h2>Project Description</h2>
            <p>This project explores the design, development, and evaluation of <strong>Accessible Digital Musical
                    Instruments (ADMIs)</strong> for users with severe motor impairments, such as quadriplegia. The
                primary goal is to empower individuals who cannot use traditional instruments by creating expressive and
                engaging musical tools that are controlled through alternative physical interaction channels, including
                eye gaze, head movement, breath, and facial gestures.</p>
            <p>Our research has produced a variety of novel instruments, from the gaze-controlled guitar
                <strong>Netytar</strong> to the accessible DJing software <strong>DJeye</strong>. At the core of this
                work is the <strong>NITH Framework</strong>, an open-source ecosystem of hardware and software we
                developed to drastically simplify and accelerate the creation of new accessible applications. By
                focusing on low-cost or DIY Open-Source hardware and modular software, the NITH framework provides a
                foundation for rapid prototyping and deep customization, enabling the creation of systems tailored to
                the unique needs and abilities of each user.
            </p>
            <p>Through case studies and user evaluations, we aim to refine our design principles and contribute to a
                future where musical expression is accessible to everyone, regardless of physical ability.</p>
        </section>

        <hr>

        <section id="instruments">
            <h2>Musical Instruments and Interfaces</h2>

            <div id="general-demos" class="indented-section">
                <h3>General Demos</h3>
                <p>Watch this video for a general demonstration of the accessible instruments in action.</p> <iframe
                    width="560" height="315" src="https://www.youtube.com/embed/owJE9MXFDjo?si=-VNUtbOof_m9CELj"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </div>

            <div id="netytar" class="indented-section">
                <h3>Netytar</h3>
                <p>Netytar is a gaze-controlled digital instrument designed for individuals with severe motor
                    disabilities. It allows users to play monophonic melodies using their eye movements for note
                    selection and breath for dynamic control, offering a high degree of expressiveness.</p>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/Netytar.webp"
                    alt="Netytar Interface" width="560" height="auto">
                <cite>The Netytar interface showing the gaze-controlled note layout.</cite><iframe width="560"
                    height="315" src="https://www.youtube.com/embed/Jf-_LkFmhbQ?si=G0vfKBaLr52nZX5P"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <h4>How it Works</h4>
                <p>The interface features an isomorphic note layout where musical intervals correspond to consistent
                    geometric shapes on the screen. While not a direct replica of a guitar fretboard, it is based on
                    similar principles of spatial relationships between notes. The user selects notes by looking at
                    specific points within this layout, and the gaze input is captured by an eye-tracker. Strumming is
                    performed through various mechanisms, such as head rotation, blinking, or using a physical switch,
                    depending on the user's abilities.</p>
                <h4>Features</h4>
                <ul>
                    <li><strong>Isomorphic, Gaze-based Note Selection:</strong> Intuitive note selection where musical
                        patterns are visually consistent.</li>
                    <li><strong>Expressive Control:</strong> Allows for techniques like hammer-ons, pull-offs, and
                        slides.</li>
                    <li><strong>Customizable Interface:</strong> The note layout and strumming mechanism can be adapted
                        to the user's needs.</li>
                </ul>
                <p><strong>Links:</strong></p>
                <ul>
                    <li><a href="https://github.com/LIMUNIMI/Netytar" target="_blank">GitHub Repository</a></li>
                </ul>
            </div>

            <div id="netychords" class="indented-section">
                <h3>Netychords</h3>
                <p>Netychords is an accessible digital musical instrument that enables users to play chords and melodies
                    using gaze control and other interaction channels. It is designed to be highly customizable to suit
                    the abilities of each user.</p>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/Netychords.png"
                    alt="Netychords Interface" width="560" height="auto">
                <cite>The Netychords interface for chord-based musical performance.</cite><iframe width="560"
                    height="315" src="https://www.youtube.com/embed/D18603o46ho?si=m16dwj9b5LIBp8Kf"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <h4>How it Works</h4>
                <p>The user selects chords by looking at keys on the screen. The strumming or playing of the chord can
                    be triggered by head rotation, blinks, or mouth movements, captured via the NITH framework's sensors
                    and wrappers. The instrument supports various mappings to provide expressive control over the music.
                </p>
                <h4>Features</h4>
                <ul>
                    <li><strong>Multi-modal Input:</strong> Combines gaze with head, blink, or mouth control.</li>
                    <li><strong>Customizable Chord Sets:</strong> Users can create and save their own chord
                        progressions.</li>
                    <li><strong>Visual Feedback:</strong> The interface provides clear visual cues for chord selection
                        and playback.</li>
                    <li><strong>Multiple Layouts:</strong> Netychords offers various chord layouts, including the
                        Stradella system (inspired by accordions) and the Flowerpot layout, to accommodate different
                        musical styles and user preferences.</li>
                </ul>
                <p><strong>Links:</strong></p>
                <ul>
                    <li><a href="https://github.com/LIMUNIMI/Netychords" target="_blank">GitHub Repository</a></li>
                </ul>
            </div>

            <div id="resin" class="indented-section">
                <h3>Resin</h3>
                <p>Resin is a monophonic Accessible Digital Musical Instrument (ADMI) that functions as a MIDI musical
                    interface, designed for users
                    with motor disabilities. It allows for the creation of complex and evolving soundscapes through
                    intuitive head motions and vocal tract shape.</p> <iframe width="560" height="315"
                    src="https://www.youtube.com/embed/fB-MRQ6_yR4?si=IoPxdsGPVgTVMfbn" title="YouTube video player"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <h4>How it Works</h4>
                <p>Resin uses two main interaction channels. Head movement along the horizontal axis (yaw) is used to
                    control note onset and dynamics (MIDI Pressure, Velocity, Note On/Off). The shape of the user's
                    vocal tract, detected through its acoustic resonances (formants), is used to control the note's
                    pitch. The user wears a head-mounted inertial measurement unit (IMU) like the NITHheadTracker. The
                    yaw, pitch, and roll of the head are mapped to various synthesis parameters, such as filter cutoff,
                    resonance, and oscillator frequency. This direct mapping allows for real-time, expressive control
                    over the sound.</p>
                <h4>Features</h4>
                <ul>
                    <li><strong>Head-Controlled Synthesis:</strong> Direct and intuitive mapping of head movements to
                        sound parameters.</li>
                    <li><strong>Vocal Tract Resonance Control:</strong> An innovative interaction method where the shape
                        of the mouth influences the pitch.</li>
                    <li><strong>Modular Design:</strong> The synthesis engine is modular, allowing users to create their
                        own custom synth patches.</li>
                    <li><strong>Low-Cost and Accessible:</strong> Built using affordable hardware and open-source
                        software.</li>
                </ul>
                <p><strong>Links:</strong></p>
                <ul>
                    <li><a href="https://github.com/LIMUNIMI/Resin" target="_blank">GitHub Repository</a></li>
                </ul>
            </div>

            <div id="kiroll" class="indented-section">
                <h3>Kiroll</h3>
                <p>Kiroll is an affordable and open-source software ADMI specifically
                    designed for quadriplegic users. It can be played by motor-impaired users through eye gaze for note
                    selection and breath for sound control.</p>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/Kiroll.webp"
                    alt="Kiroll Interface" width="560" height="auto">
                <cite>Kiroll's interface showing the infinite keyboard context-switching paradigm.</cite>
                <h4>How it Works</h4>
                <p>Kiroll features an innovative "infinite keyboards" context-switching interaction method. The
                    interface displays multiple parallel, vertically-arranged keyboards that scroll horizontally. The
                    musician selects a key on the leftmost keyboard with their gaze and then blows into a breath sensor
                    (like the NITHbreathSensor) to play the note. To play the next note, the user shifts their gaze to a
                    key on the adjacent keyboard. This design avoids the "Midas Touch" problem common in gaze-based
                    interfaces, where unintentional notes are triggered as the gaze moves across the screen. The
                    interface automatically scrolls, allowing for a seamless and continuous playing experience, similar
                    to reading a musical staff.</p>
                <h4>Features</h4>
                <ul>
                    <li><strong>Gaze-based Note Selection with Breath Control:</strong> An intuitive interaction
                        paradigm that separates note selection from sound production.</li>
                    <li><strong>Infinite Keyboards and Autoscrolling:</strong> An endlessly scrolling layout that
                        resolves the Midas Touch issue and allows for continuous play.</li>
                    <li><strong>Customizable Interface:</strong> Users can adjust the musical scale, octave, and the
                        distance between keyboards.</li>
                    <li><strong>Open-Source:</strong> Kiroll and its source code are available under a GNU GPL-v3
                        license.</li>
                </ul>
                <p><strong>Links:</strong></p>
                <ul>
                    <li><a href="https://github.com/LIMUNIMI/Kiroll" target="_blank">GitHub Repository</a></li>
                </ul>
            </div>

            <div id="djeye" class="indented-section">
                <h3>DJeye</h3>
                <p>DJeye is an innovative, accessible musical interface designed for DJing, addressing the challenge
                    that this activity poses for individuals with limited motor capabilities like quadriplegia. The
                    system is a software-based musical interface controlled by eye tracking, which allows for typical DJ
                    mixing operations.</p><iframe width="560" height="315"
                    src="https://www.youtube.com/embed/-bs08Ohdr7w?si=GiCkn4RONwiUbmtO" title="YouTube video player"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <h4>How it Works</h4>
                <p>DJeye functions as a MIDI controller that sends commands to an underlying, free open-source DJing
                    software called MIXXX. The user interacts with the DJeye interface using only gaze pointing and
                    winking (blinking one eye). The system consists of two main software components: an <strong>Eye
                        Interaction Layer</strong>, which translates raw data from an eye tracker (e.g., a Tobii device)
                    into mouse events, and a <strong>DJ Interface Layer</strong>, which provides the graphical
                    controller. A left wink triggers a click, while closing and holding the right eye allows the user to
                    "drag" sliders up or down by moving their gaze.</p>
                <h4>Features</h4>
                <ul>
                    <li><strong>Gaze-Based Control:</strong> The entire interface is controlled via eye movements and
                        winks.</li>
                    <li><strong>Essential DJ Functions:</strong> Allows for crossfading, volume control, high/low-pass
                        filtering, looping, track seeking, and pre-listening on headphones.</li>
                    <li><strong>Radial Interface Design:</strong> The controls for each deck are arranged in a circular,
                        pie-shaped layout around a central play/pause button, inspired by interfaces like The EyeHarp.
                        This design keeps all elements close together and enlarges the focused deck for easier
                        interaction.</li>
                    <li><strong>MIXXX Integration:</strong> Leverages the free and open-source software MIXXX for the
                        underlying audio engine and track management.</li>
                    <li><strong>Open Source:</strong> The software is published under a GNU-GPL v3 license and is built
                        using the C++ JUCE framework for cross-platform compatibility.</li>
                </ul>
                <p><strong>Links:</strong></p>
                <ul>
                    <li><a href="https://github.com/LIMUNIMI/DJeye" target="_blank">GitHub Repository</a></li>
                    <li><a href="https://youtu.be/-bs08Ohdr7w?si=GiCkn4RONwiUbmtO" target="_blank">YouTube Demo
                            Video</a></li>
                </ul>
            </div>
        </section>

        <hr>

        <section id="installation-tutorials">
            <h2>Installation Tutorials</h2>
            <div class="indented-section">
                <h3>Instrument Installation</h3>
                <p>For detailed installation instructions, please refer to the README files in the official GitHub
                    repositories for each instrument, linked in their respective sections above. The `README.md` file in
                    each repository provides the necessary steps to get the software running.</p>
                <p>Additionally, this video tutorial provides a step-by-step guide on how to download and set up our
                    accessible musical instruments.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/cuS3y8yHozM?si=OMlgK1l68pkMUGIG"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </div>
        </section>

        <hr>
        <section id="nith-framework">
            <h2>The NITH Framework</h2>
            <p>The NITH framework is a comprehensive software and hardware ecosystem designed to facilitate the
                development of accessible applications for users with quadriplegia and other hand impairments. It offers
                a collection of sensory peripherals, software tools, and libraries that support a wide range of
                interaction channels, such as eye, head, and tongue movements, as well as voice, breath, and facial
                gestures.</p>

            <p>The framework is a modular platform for designing and prototyping accessible human–computer
                interaction systems tailored for users with disabilities that impair the use of hands and feet as means
                of interaction. It is built on the core principles of <strong>simplicity, modularity, and
                    affordability</strong> to empower developers, caregivers, and users themselves.</p>

            <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/NithLogo_White_Trace.png"
                alt="NITH framework logo" style="max-width: 200px;" width="560" height="auto">
            <cite>The NITH framework logo.</cite>

            <h3>Philosophy</h3>
            <p>NITHsensors designs were developed with a specific philosophy in mind:</p>
            <ul>
                <li><strong>Affordability:</strong> All hardware designs use common, readily available components</li>
                <li><strong>Accessibility:</strong> No specialized manufacturing skills required for assembly</li>
                <li><strong>Open Source:</strong> All software released under open-source licenses and hardware designs
                    under free licenses</li>
                <li><strong>KISS Principle:</strong> Keep It Simple, Stupid - emphasizing simplicity in design and
                    implementation</li>
                <li><strong>Modularity:</strong> Standard communication protocol allows easy integration and
                    customization</li>
            </ul>

            <h3>Uses and Applications</h3>
            <p>The NITHsensors are a product of research into developing accessible musical instruments for individuals
                with quadriplegia. The instruments are operated by NITHsensors alone or in combination with other
                commercially available peripherals. However, as music is not the only important aspect of human-machine
                interaction — there's
                potential for using these solutions in other contexts, such as developing applications for general
                computer accessibility.</p>

            <h3>Interaction Channels Overview</h3>
            <p>Below is a summary table, indicating which channels are already detectable with NITHsensors devices
                and/or with commercial alternatives:</p>

            <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/InteractionChannels_ABCD.png"
                alt="Alternative interaction channels diagram" width="560" height="auto">
            <cite>The areas in which we subdivided the various interaction channels (A: Eyes, B: Mouth, C: Head, D:
                Brain) used in accessible digital musical instruments.</cite>

            <table style="width: 100%; border-collapse: collapse; margin: 2em 0;">
                <thead>
                    <tr style="background-color: #f5f5f5;">
                        <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Region</th>
                        <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Parameter</th>
                        <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Description</th>
                        <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">NITHsensors</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;" rowspan="12"><strong>Eyes</strong></td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Left Eye Aperture</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Continuous measure of how open the left eye is
                        </td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Right Eye Aperture</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Continuous measure of how open the right eye
                            is</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Left Eye Status</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Boolean indicating if the left eye is open or
                            closed</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper (+ postprocessor)</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Right Eye Status</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Boolean indicating if the right eye is open or
                            closed</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper (+ postprocessor)</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Left Eye Position</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">3D position of the left eye in space</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Right Eye Position</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">3D position of the right eye in space</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Left Eyebrow Height</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Height of the left eyebrow</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Right Eyebrow Height</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Height of the right eyebrow</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Left Eyebrow Phase</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Phase of the left eyebrow (down, neutral, up)
                        </td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper (+ preprocessor)</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Right Eyebrow Phase</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Phase of the right eyebrow (down, neutral, up)
                        </td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper (+ preprocessor)</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Gaze Coordinates</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">X and Y coordinates of the user's gaze on the
                            screen</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHbeamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Gaze Presence</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Boolean indicating if gaze is being detected
                        </td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHbeamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;" rowspan="8"><strong>Mouth</strong></td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Voice Pitch</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Voice pitch frequency</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">-</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Voice Intensity</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Voice intensity (volume)</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">-</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Whistle Pitch</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Whistle pitch (frequency)</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">-</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Whistle Intensity</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Whistle intensity (volume)</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">-</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Breath Pressure</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Air pressure from breathing or blowing</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHbreathSensor</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Mouth Aperture</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Continuous measure of how open the mouth is
                        </td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Mouth Status</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Boolean indicating if the mouth is open or
                            closed</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper (+ preprocessor)</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Teeth Pressure</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Pressure from teeth clenching or jaw movement
                        </td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHbiteSensor</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;" rowspan="3"><strong>Head</strong></td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Head Presence</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Boolean indicating if a head is being detected
                        </td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Head Position</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Head rotation (yaw, pitch, roll)</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHheadTracker, NITHwebcamWrapper</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Head Acceleration</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Head rotational acceleration (yaw, pitch,
                            roll)</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">NITHheadTracker, NITHwebcamWrapper (+
                            preprocessor)</td>
                    </tr>
                </tbody>
            </table>

            <h3>Brain-Computer Interfaces (BCI)</h3>
            <p>Brain-Computer Interfaces (BCIs) represent another potential category of interaction channels, which
                involve using brain signals to control computers or other devices. These signals are typically captured
                using non-invasive sensors like electroencephalography (EEG) headsets.</p>
            <p>While powerful, the complexity and high cost of current BCI technology make it challenging to integrate
                into DIY and low-cost frameworks like NITH. For this reason, the NITH framework does not currently offer
                support for BCI peripherals. However, it remains a promising area for future exploration.</p>

            <h3>Developing Applications Using NITHsensors</h3>
            <p>All NITHsensors send a standard data string each time they perform a sampling. The output of all
                NITHsensors uses a very simple standard format. This format can be seen in the <a
                    href="https://github.com/LIMUNIMI/NITHsensors" target="_blank">README of the GitHub repository</a>
                under the <em>Standard Output</em> section.</p>

            <p>The software library NITHdmis contains several modules capable of interpreting the input and easily
                developing applications in C#. The communication standard is very simple, making it equally simple to
                write input interpreters for other programming languages.</p>

            <div id="nithsensors" class="indented-section">
                <h3>NITHsensors (Hardware)</h3>
                <p><em>NITHsensors</em> is a collection of do-it-yourself (DIY), open-source hardware sensors. They are
                    designed to be easy and affordable to build and customize, without requiring specialized
                    manufacturing
                    skills.</p>

                <h4>NITHbreathSensor</h4>
                <p>This sensor detects breath pressure and airflow, allowing for nuanced control similar to a
                    sip-and-puff
                    device. It can be assembled for approximately <strong>€30 and €35</strong> using an Arduino, a
                    low-pressure air sensor (e.g., MPX 5010DP), and common tubing. The design includes a washable,
                    interchangeable mouthpiece and ensures no soldering is required.</p>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/NITHbreathSensor.webp"
                    alt="Assembled NITHbreathSensor" width="560" height="auto">
                <cite>A photo of a built sample of the NITHbreathPressureSensor.</cite>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/NITHbreathSensor_schematics.webp"
                    alt="NITHbreathSensor schematics" width="560" height="auto">
                <cite>Schematics for assembling the NITHbreathSensor.</cite>
                <h5>Output Parameters</h5>
                <ul>
                    <li><strong>Breath Pressure:</strong> A continuous value representing the air pressure detected by
                        the sensor.</li>
                </ul>

                <h4>NITHheadTracker</h4>
                <p>This device measures head orientation and angular acceleration along the yaw, pitch, and roll axes.
                    It
                    can be used as a high-precision pointing device for tasks like mouse emulation. The estimated cost
                    is
                    approximately <strong>€40</strong>, using an Arduino and a BNO055 accelerometer/gyroscope board
                    mounted
                    on a simple headset or hairband. The integrated magnetometer ensures that the positional data does
                    not
                    drift over time.</p>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/NITHheadTracker.png"
                    alt="Assembled NITHheadTracker" width="560" height="auto">
                <cite>A photo of a built sample of the NITHheadTracker, mounted on a headband.</cite>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/NITHheadTracker_schematics.png"
                    alt="NITHheadTracker schematics" width="560" height="auto">
                <cite>Schematics for assembling the NITHheadTracker.</cite>
                <h5>Output Parameters</h5>
                <ul>
                    <li><strong>Head Rotation Position:</strong> The absolute rotational position of the head on the
                        yaw, pitch, and roll axes.</li>
                    <li><strong>Head Rotation Acceleration:</strong> The rotational acceleration of the head on the yaw,
                        pitch, and roll axes.</li>
                    <li><strong>Calibration Status:</strong> The calibration status for the system, gyroscope,
                        accelerometer, and magnetometer.</li>
                </ul>

                <h4>NITHbiteSensor</h4>
                <p>This sensor is designed to detect dental pressure using Force-Sensing Resistors (FSRs), enabling
                    control
                    through teeth clenching or tapping. It is an affordable option, costing approximately
                    <strong>€20-25</strong> to build. The assembly involves an Arduino and an FSR sensor mounted on a
                    small,
                    flat material like a wooden stick.
                </p>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/NITHbiteSensor.png"
                    alt="Assembled NITHbiteSensor" width="560" height="auto">
                <cite>A photo of a built sample of the NITHbiteSensor.</cite>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/NITHbiteSensor_schematics.png"
                    alt="NITHbiteSensor schematics" width="560" height="auto">
                <cite>Schematics for assembling the NITHbiteSensor.</cite>
                <h5>Output Parameters</h5>
                <ul>
                    <li><strong>Teeth Pressure:</strong> A continuous value representing the pressure detected by the
                        sensor.</li>
                </ul>
            </div>

            <div id="nithsoftware" class="indented-section">
                <h3>NITHwrappers & Libraries (Software)</h3>
                <p>The software ecosystem of NITH provides the tools to interface with sensors, process data, and build
                    applications.</p>

                <h4>Communication Protocol</h4>
                <p>All NITH peripherals communicate using a simple, standardized protocol. This ensures that any sensor
                    or
                    wrapper can seamlessly connect to any application built with the NITHlibrary. The protocol is
                    designed
                    to be easy to implement, allowing developers to quickly integrate new custom-built peripherals.</p>

                <h4>NITHwrappers</h4>
                <p><em>NITHwrappers</em> are software components that allow commercially available sensors to be used
                    with
                    the NITH framework.</p>
                <ul>
                    <li><strong>NITHwebcamWrapper</strong>: A Python script that uses Google's MediaPipe framework to
                        turn
                        any standard webcam into a facial movement tracker. It can detect head rotation, eye aperture
                        (for
                        blinks), and mouth opening.
                        <h5>Output Parameters</h5>
                        <ul>
                            <li><strong>Head Position:</strong> The position of the head in degrees.</li>
                            <li><strong>Eye Aperture Ratios:</strong> The aperture ratio for the left and right eyes.
                            </li>
                            <li><strong>Mouth Aperture Ratio:</strong> The aperture ratio for the mouth.</li>
                        </ul>
                    </li>
                    <li><strong>NITHbeamWrapper</strong>: A wrapper for the Beam Eye Tracker software, allowing a
                        standard
                        webcam to be used for gaze tracking and head pose estimation within the NITH ecosystem.
                        <h5>Output Parameters</h5>
                        <ul>
                            <li><strong>Head Rotation:</strong> The rotational data of the user's head.</li>
                            <li><strong>Gaze Position:</strong> The user's gaze position on the screen.</li>
                            <li><strong>Head Presence:</strong> A boolean indicating if the head is detectable.</li>
                            <li><strong>Gaze Presence:</strong> A boolean indicating if the gaze is detectable.</li>
                        </ul>
                    </li>
                </ul>
                <img src="https://raw.githubusercontent.com/LIMUNIMI/risorse-pagina-accessibilita/refs/heads/main/resources/NITHwebcamWrapper.png"
                    alt="NITHwebcamWrapper in action" width="560" height="auto">
                <cite>The NITHwebcamWrapper detecting facial movements using MediaPipe.</cite>

                <h4>Core Libraries</h4>
                <p>The core libraries are written in C# using .NET 8, ensuring cross-platform compatibility.</p>
                <ul>
                    <li><strong>NITHlibrary</strong>: The main library that provides classes for parsing input from all
                        NITH
                        sensors and wrappers. It includes tools for data manipulation, filtering, and calibration.</li>
                    <li><strong>NITHemulation</strong>: An extension that provides tools to emulate standard inputs,
                        such as
                        mouse movements, clicks, and keyboard events.</li>
                    <li><strong>NITHdmis</strong>: A specialized extension with tools specifically for developing
                        Accessible
                        Digital Musical Instruments (ADMIs).</li>
                </ul>

                <h4>Development Tools</h4>
                <p>To accelerate development, the framework includes several helpful tools:</p>
                <ul>
                    <li><strong>NITHtester</strong>: A graphical application for testing sensors and wrappers. It
                        provides
                        real-time visualization of sensor data, status codes, and errors through gauges
                        and plots.</li>
                    <li><strong>NITHtemplate</strong>: A C# project template that provides a ready-made structure for
                        new NITH applications, suggesting a design philosophy that simplifies development.</li>
                </ul>
            </div>
        </section>

        <section id="publications" style="margin-top: 3em;">
            <h2>Publications</h2>
            <ul>
                <li>Davanzo, Nicola, Piercarlo Dondi, Mauro Mosconi, and Marco Porta. “Playing Music with the Eyes
                    through an Isomorphic Interface.” In <em>Proc. of the Workshop on Communication by Gaze
                        Interaction</em>, 1–5. Warsaw, Poland: ACM Press, 2018. <a
                        href="https://doi.org/10.1145/3206343.3206350"
                        target="_blank">https://doi.org/10.1145/3206343.3206350</a>.</li>
                <li>Davanzo, Nicola, and Federico Avanzini. “A Dimension Space for the Evaluation of Accessible Digital
                    Musical Instruments.” In <em>Proc. 20th Int. Conf. on New Interfaces for Musical Expression (NIME
                        ’20)</em>. NIME ’20, 2020. <a href="https://doi.org/10.5281/ZENODO.4813326"
                        target="_blank">https://doi.org/10.5281/ZENODO.4813326</a>.</li>
                <li>Davanzo, Nicola, and Federico Avanzini. “A Method for Learning Netytar: An Accessible Digital
                    Musical Instrument.” In <em>Proceedings of the 12th International Conference on Computer Supported
                        Education</em>, 620–28. Prague, Czech Republic: SCITEPRESS - Science and Technology
                    Publications, 2020. <a href="https://doi.org/10.5220/0009816106200628"
                        target="_blank">https://doi.org/10.5220/0009816106200628</a>.</li>
                <li>Davanzo, Nicola, and Federico Avanzini. “Experimental Evaluation of Three Interaction Channels for
                    Accessible Digital Musical Instruments.” In <em>Proc. ’20 Int. Conf. on Computers Helping People
                        With Special Needs</em>, 437–45. Online Conf.: Springer, Cham, 2020.</li>
                <li>Davanzo, Nicola, and Federico Avanzini. “Hands-Free Accessible Digital Musical Instruments:
                    Conceptual Framework, Challenges, and Perspectives.” <em>IEEE Access</em> 8 (2020): 163975–95. <a
                        href="https://doi.org/10.1109/ACCESS.2020.3019978"
                        target="_blank">https://doi.org/10.1109/ACCESS.2020.3019978</a>.</li>
                <li>Davanzo, Nicola, Matteo De Filippis, and Federico Avanzini. “Netychords: An Accessible Digital
                    Musical Instrument for Playing Chords Using Gaze and Head Movements.” In <em>In Proc. ’21 Int. Conf.
                        on Computer- Human Interaction Research and Applications (CHIRA ’21)</em>, 8. Online conf.:
                    SciTePress, 2021.</li>
                <li>Davanzo, Nicola, and Federico Avanzini. “Resin: A Vocal Tract Resonances and Head Based Accessible
                    Digital Musical Instrument.” In <em>Proceedings of the 2021 AudioMostly Conf</em>. Trento, Italy
                    (online conf.), 2021. <a href="https://doi.org/10.1145/3478384.3478403"
                        target="_blank">https://doi.org/10.1145/3478384.3478403</a>.</li>
                <li>Davanzo, Nicola, and Federico Avanzini. “Design Concepts for Gaze-Based Digital Musical
                    Instruments.” In <em>Proceedings of the 2022 Sound and Music Computing Conference</em>, 477–83.
                    Saint-Etiénne, France: Zenodo, 2022.</li>
                <li>Davanzo, Nicola. “Accessible Digital Musical Instruments for Quadriplegic Musicians.” Ph. D. Thesis,
                    Università degli Studi di Milano, 2022.</li>
                <li>Bottarelli, Fabio, Nicola Davanzo, Giorgio Presti, and Federico Avanzini. “DJeye: Towards an
                    Accessible Gaze-Based Musical Interface for Quadriplegic DJs.” In <em>Proceedings of the 2023 Sound
                        and Music Computing Conference</em>. Stockholm, Sweden, 2023.</li>
                <li>Davanzo, Nicola, Federico Avanzini, Luca A Ludovico, Davys Moreno, Antonio Moreira, Julia Azevedo,
                    and Carlos Marques. “A Case Study on Netychords: Crafting Accessible Digital Musical Instrument
                    Interaction for a Special Needs Scenario.” In <em>Proc. ’23 Conf. on Computer-Human Interaction
                        Research and Applications</em>. Rome, Italy: Springer, 2023.</li>
                <li>Davanzo, Nicola, Luca Valente, Luca A. Ludovico, and Federico Avanzini. “Kiroll: A Gaze-Based
                    Instrument for Quadriplegic Musicians Based on the Context-Switching Paradigm.” In <em>Proc. ’23
                        AudioMostly Conference</em>. Edinburgh, Scotland: ACM, 2023.</li>
                <li>Moreno, Davys, Júlia Azevedo, Bernardo Lima, and Nicola Davanzo. “Music for All: An Intervention
                    Project in an Artistic School in Portugal.” <em>The Qualitative Report</em> 28, no. 10 (October 14,
                    2023): 2953–79. <a href="https://doi.org/10.46743/2160-3715/2023.6682"
                        target="_blank">https://doi.org/10.46743/2160-3715/2023.6682</a>.</li>
                <li>Guillen, Sergio, Isabel Barbancho, Lorenzo J Tardon, Ana M Barbancho, and Nicola Davanzo.
                    “Understanding Music-Related Brain Activity: Musical Pitch Processing through EEG Signals,” (to be
                    published, conference to be held).
                </li>
            </ul>
        </section>

        <footer style="margin-top: 3em; padding-top: 2em; border-top: 1px solid #eee;">
            <p style="font-size: 0.9em; color: #777;">© 2025 LIM - Laboratorio di Informatica Musicale. All rights
                reserved.</p>
        </footer>
    </div>

</body>

</html>